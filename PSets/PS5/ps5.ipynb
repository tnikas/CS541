{"cells":[{"cell_type":"markdown","id":"RtyaMANFEZKH","metadata":{"id":"RtyaMANFEZKH"},"source":["# CS541: Applied Machine Learning, Spring 2025, Problem Set 5\n","\n","### _Note: Do not delete or add blocks, do not change the function names. If you do this the autograder won't run properly and you might get a 0. Avoid using global variables inside your function. Failing to define your variables correctly could result in a score of 0._\n","\n","\n","Problem set 5 is due in Gradescope on **Apr 29th at 11:59pm**.\n","All the questions are in this jupyter notebook file. There are four questions in this assignment, each of which could have multiple parts and consists of a mix of coding and short answer questions. This assignment is worth a total of **95 points** (**52.5 pts** coding, and **42.5 pts** short answer).\n","\n","After completing these questions you will need to covert this notebook into a .py file named **ps5.py** and a pdf file named **ps5.pdf** in order to submit it (details below).\n","\n","\n","**Submission instructions:** please upload your completed solution files to Gradescope by the due date. **Make sure you have run all code cells and rendered all markdown/Latex without any errors.**\n","\n","There will be three separate submission links for the assignment:\n","1. Submit **ps5.py** to `PS5-Code`\n","2. Submit **ONLY your typed code** to `PS5-Typed Code`.\n","  + The .py file should contain **ONLY your typed code** (Do not include any other code apart from what you coded for the assignment).\n","  + The .py should not contain any written answers. Only the code you wrote.\n","  + If your typed code falls under a function definition thats predefined by us, **ONLY include your typed code** and nothing else.\n","  + For each cell block within colab/jupyter that you typed your ocde in, Add 2 new lines before pasting your typed code in the .py file.\n","  + Please name the .py file your actual name.\n","\n","3. Submit a single `.pdf` report that contains your work for all written questions to `PS5`. You can type your responses in LaTeX, or any other word processing software.  You can also hand write them on a tablet, or scan in hand-written answers. If you hand-write, please make sure they are neat and legible. If you are scanning, make sure that the scans are legible. Lastly, convert your work into a `PDF`. You can use Jupyter Notebook to convert the formats:\n","  + Convert to PDF file: Go to File->Download as->PDF\n","  + Convert py file: Go to File->Download as->py\\\n","You can take a look at an example [here](https://raw.githubusercontent.com/chaudatascience/cs599_fall2022/master/ps1/convert_py.gif)\n","\n","  Your written responses in the PDF report should be self-contained. It should include all the output you want us to look at. **You will not receive credit for any results you have obtained, but failed to include directly in the PDF report file.  Please tag the reponses in your PDF with the Gradescope questions outline  as described in [Submitting an Assignment](https://youtu.be/u-pK4GzpId0). Failure to follow these instructions will result in a loss of points.**\n","\n","  \n","\n"]},{"cell_type":"markdown","id":"a636aea4","metadata":{"id":"a636aea4"},"source":["**Assignment Setup**\n","\n","You are strongly encouraged to use [Google Colab](https://colab.research.google.com/) for this assignment.\n","\n","If you would prefer to setup your code locally on your own machine, you will need [Jupyter Notebook](https://jupyter.org/install#jupyter-notebook) or [JupyterLab](https://jupyter.org/install#jupyterlab) installation. One way to set it up is to install “Anaconda” distribution, which has Python (you should install python version >= 3.9 as this notebook is tested with python 3.9), several libraries including the Jupyter Notebook that we will use in class. It is available for Windows, Linux, and Mac OS X [here](https://docs.conda.io/en/latest/miniconda.html).\n","\n","If you are not familiar with Jupyter Notebook, you can follow [this blog](https://realpython.com/jupyter-notebook-introduction/) for an introduction.  After developing your code using Jupyter, you are encouraged to test it on Google Colab to ensure it works in both settings.\n","\n","\n","You cannot use packages other than the ones already imported in this assignment."]},{"cell_type":"markdown","id":"49330a02","metadata":{"id":"49330a02"},"source":["**Jupyter Tip 1**: To run a cell, press `Shift+Enter` or click on \"play\" button above. To edit any code or text cell [double] click on its content. To change cell type, choose \"Markdown\" or \"Code\" in the drop-down menu above."]},{"cell_type":"markdown","id":"137191ca","metadata":{"id":"137191ca"},"source":["**Jupyter Tip 2**: Use shortcut \"Shift + Tab\" to show the documentation of a function in Jupyter Notebook/ Jupterlab. Press Shift then double Tab (i.e., press Tab twice) to show the full documentation.\\\n","For example, type `sum(` then Shift + Tab to show the documentation for the function, as shown in this the picture below."]},{"cell_type":"code","execution_count":null,"id":"557d9f4e","metadata":{"id":"557d9f4e"},"outputs":[],"source":["## import some libraries\n","import numpy as np\n","from typing import Tuple, List, Dict\n","import pandas as pd\n","import matplotlib\n","import matplotlib.pyplot as plt\n","from collections import namedtuple\n","from IPython.display import display\n","import random\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader, Dataset"]},{"cell_type":"markdown","id":"283c5c3f","metadata":{"id":"283c5c3f"},"source":["# **Question 1.** Autoencoder (*45 total points*)"]},{"cell_type":"markdown","id":"281c0caf","metadata":{"id":"281c0caf"},"source":["In this question, you will build and train an autoencoder model using the MNIST dataset. The MNIST dataset is a collection grayscale images of handwritten digits ranging from 0 to 9 commonly used for image classification tasks.\n","\n","An autoencoder is a neural network that learns to compress the input data into a lower-dimensional representation (encoding) and then reconstruct it back to its original form (decoding). It consists of the following two major components:\n","\n","**Encoder**: compresses the input images into lower-dimensional representation.\n","\n","**Decoder**: takes the encoded representation and reconstructs an output as close as possible to the original input image.\n"]},{"cell_type":"markdown","id":"3a0b40e5","metadata":{"id":"3a0b40e5"},"source":["## **1.1 Code:** Data Transformation *(2.5 pts)*"]},{"cell_type":"markdown","id":"e0b87bc5","metadata":{"id":"e0b87bc5"},"source":["For the following sections, we will work on [MNIST](https://www.kaggle.com/datasets/hojjatk/mnist-dataset) dataset.\n","\n","\n","\n","First, let's download and preprocess the dataset"]},{"cell_type":"code","execution_count":null,"id":"fc10a1e4","metadata":{"id":"fc10a1e4"},"outputs":[],"source":["def question_1_1() -> transforms.Compose:\n","    \"\"\"\n","    Create and return a transformation pipeline for image preprocessing.\n","    \"\"\"\n","    # Write your code in this block -----------------------------------------------------------\n","\n","    ## Step 1: Use transforms.Compose\n","    # 1. resize images to 28x28 pixels.\n","    # 2. convert the image to a PyTorch tensor.\n","\n","\n","    ## Return the transformation pipeline\n","    return transform\n","    # End of your code -----------------------------------------------------------\n","\n"]},{"cell_type":"markdown","id":"e5182760","metadata":{"id":"e5182760"},"source":["We will now create the dataloaders for training and testing."]},{"cell_type":"code","execution_count":null,"id":"4756cc00","metadata":{"id":"4756cc00"},"outputs":[],"source":["\n","# Load the MNIST dataset for training and testing using batch size of 16\n","transform = question_1_1()\n","train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n","test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n","\n","train_loader = DataLoader(dataset=train_dataset, batch_size=16, shuffle=True)\n","test_loader = DataLoader(dataset=test_dataset, batch_size=16, shuffle=False)\n"]},{"cell_type":"markdown","id":"5281f3f6","metadata":{"id":"5281f3f6"},"source":["## **1.2 Code:** Constructing Autoencoder *(5 pts)*"]},{"cell_type":"code","execution_count":null,"id":"a703f677","metadata":{"id":"a703f677","outputId":"bb030a9a-3b41-4d7e-b1be-625cac9fd86e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Autoencoder(\n","  (encoder): Sequential(\n","    (0): Linear(in_features=784, out_features=128, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=128, out_features=64, bias=True)\n","  )\n","  (decoder): Sequential(\n","    (0): Linear(in_features=64, out_features=128, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=128, out_features=784, bias=True)\n","    (3): Sigmoid()\n","  )\n",")\n"]}],"source":["import torch.nn as nn\n","from torch import Tensor\n","\n","class Autoencoder(nn.Module):\n","    def __init__(self, encoding_dim: int):\n","\n","        super().__init__()\n","\n","        # Write your code in this block -----------------------------------------------------------\n","\n","        # Initialize self.encoder and self.decoder based on the given output\n","        # (note the dimension for in_features and out_features)\n","\n","        # Construct encoder using following steps:\n","        # 1. Apply a linear layer with 784 input features and 128 output features.\n","        # 2. Add a ReLU activation function.\n","        # 3. Apply a second linear layer that reduces the representation from 128 features to encoding_dim\n","\n","\n","        # Construct decoder using following steps:\n","        # 1. Apply a linear layer with encoding_dim features and 128 output features.\n","        # 2. Add a ReLU activation function.\n","        # 3. Apply a second linear layer from 128 features to 784 features\n","        # 4. Apply a sigmoid layer.\n","\n","\n","\n","\n","    def forward(self, x: Tensor) -> Tensor:\n","        \"\"\"\n","        Forward pass for the autoencoder.\n","        x: Input tensor of shape (batch_size, 1, 28, 28).\n","        Returns the reconstructed tensor of the same shape (batch_size, 1, 28, 28)..\n","        \"\"\"\n","        # Flatten the input image to shape (batch_size,28*28)\n","\n","\n","\n","\n","        # End of your code ------------------------------------------------------------------------\n","\n","# Example of instantiating the model\n","encoding_dim = 64\n","model = Autoencoder(encoding_dim)\n","print(model)"]},{"cell_type":"markdown","id":"b1274691","metadata":{"id":"b1274691"},"source":["For training we need to first create `model` and set its device. We will use GPU if it's available, otherwise CPU.  For `optimizer` we will use Adam and set the initial learning rate as 0.001."]},{"cell_type":"code","execution_count":null,"id":"fa87918c","metadata":{"id":"fa87918c"},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","encoding_dim = 64\n","model = Autoencoder(encoding_dim).to(device)\n","optimizer = optim.Adam(model.parameters(), lr=0.001)"]},{"cell_type":"markdown","id":"3311df93","metadata":{"id":"3311df93"},"source":["## **1.3 Code:** Training Autoencoder *(5 pts)*"]},{"cell_type":"markdown","id":"6a078060","metadata":{"id":"6a078060"},"source":["Now, let us write the function to train an autoencoder model."]},{"cell_type":"code","execution_count":null,"id":"f4043e75","metadata":{"id":"f4043e75"},"outputs":[],"source":["def question_1_3(model: nn.Module, dataloader: DataLoader, criterion, optimizer, epochs) -> None:\n","    \"\"\"\n","    Train an autoencoder model on a given dataset.\n","\n","    Args:\n","    - model (nn.Module): The autoencoder model to be trained\n","    - dataloader (DataLoader): Dataloader for trianing\n","    - criterion: Loss function\n","    - optimizer: Optimizer\n","    - epochs: NUmber of trianing epochs\n","\n","    \"\"\"\n","\n","\n","    # Write your code in this block -----------------------------------------------------------\n","    ## Set training mode\n","    model.train()\n","\n","    ## Loop through each epoch\n","    for epoch in range(epochs):\n","        total_loss = 0.0\n","        for images, _ in dataloader:\n","            images = images.to(device)\n","\n","            # Forward pass\n","\n","\n","            # Zero the gradients, backpropagate, and update the weights\n","\n","\n","            # Update the loss\n","\n","\n","        # Compute average loss over the epoch  and print out the loss\n","        avg_loss = total_loss / len(dataloader)\n","        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}\")\n","    # End of your code ------------------------------------------------------------------------\n"]},{"cell_type":"markdown","id":"7fd2c740","metadata":{"id":"7fd2c740"},"source":["## **1.4 Short answer:**  Deciding Loss Function *(2.5 pts)*"]},{"cell_type":"markdown","id":"708a4d55","metadata":{"id":"708a4d55"},"source":["When training the model, experiment with different loss functions as the criterion. Choose the most suitable loss function for the task and explain why it is the best choice."]},{"cell_type":"code","execution_count":null,"id":"ea241923","metadata":{"id":"ea241923"},"outputs":[],"source":["# Write your code in this block -----------------------------------------------------------\n","# invoke question_1_3 here, try out at least 2 loss functions, and leave one for the test set below\n","\n","\n","\n","criterion =\n","\n","train_model = question_1_3"]},{"cell_type":"markdown","id":"aefa2556","metadata":{"id":"aefa2556"},"source":["Write your answer in this block\n","\n","**Answer:**"]},{"cell_type":"markdown","id":"789e3086","metadata":{"id":"789e3086"},"source":["Now, let us evaluate the performance of the autoencoder using the test set."]},{"cell_type":"code","execution_count":null,"id":"05ead8d9","metadata":{"id":"05ead8d9"},"outputs":[],"source":["def evaluate_model(model: nn.Module, dataloader: DataLoader) -> float:\n","    \"\"\"\n","    Evaluate an autoencoder model on a given dataset.\n","\n","    Args:\n","    - model (nn.Module): The autoencoder model to be evaluated\n","    - dataloader (DataLoader): Dataloader for evaluation\n","\n","    Returns:\n","    - float: Average loss over the evaluation dataset\n","    \"\"\"\n","\n","    ## Set the model to evaluation mode\n","    model.eval()\n","    total_loss = 0.0\n","\n","    ## Disable gradient calculation\n","    with torch.no_grad():\n","        for images, _ in dataloader:\n","            images = images.to(device)\n","            outputs = model(images)\n","            loss = criterion(outputs, images)\n","\n","    # Calculate total_loss\n","            total_loss += loss.item()\n","\n","    # Calculate average loss over the entire evaluation dataset and print out the loss\n","    avg_loss = total_loss / len(dataloader)\n","    print(f\"Test Loss: {avg_loss:.4f}\")\n","\n","\n","\n","    return avg_loss\n"]},{"cell_type":"code","execution_count":null,"id":"e285359e","metadata":{"id":"e285359e","outputId":"3cffb4f6-440b-4972-f8a2-27588abb9126"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch [1/5], Loss: 0.0206\n","Epoch [2/5], Loss: 0.0086\n","Epoch [3/5], Loss: 0.0066\n","Epoch [4/5], Loss: 0.0055\n","Epoch [5/5], Loss: 0.0049\n","Test Loss: 0.0046\n"]},{"data":{"text/plain":["0.00462578858807683"]},"execution_count":35,"metadata":{},"output_type":"execute_result"}],"source":["question_1_3(model, train_loader, criterion, optimizer, epochs=5)\n","evaluate_model(model, test_loader)"]},{"cell_type":"markdown","id":"18f3da1e","metadata":{"id":"18f3da1e"},"source":["To visualize the performance of the current autoencoder run the following block:"]},{"cell_type":"code","execution_count":null,"id":"7202d745","metadata":{"id":"7202d745","outputId":"cc37173d-30f4-4e92-c720-73d539f0c183"},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 2500x400 with 20 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["import matplotlib.pyplot as plt\n","dataiter = iter(test_loader)\n","images, labels = next(dataiter)\n","\n","images_flatten = images.view(images.size(0), -1)\n","output = model(images_flatten)\n","images = images.numpy()\n","\n","\n","output = output.view(16, 1, 28, 28)\n","output = output.detach().numpy()\n","\n","fig, axes = plt.subplots(nrows=2, ncols=10, sharex=True, sharey=True, figsize=(25, 4))\n","\n","row_titles = [\"Actual\", \"Predicted\"]\n","for images_set, row, title in zip([images, output], axes, row_titles):\n","    for img, ax in zip(images_set, row):\n","        ax.imshow(np.squeeze(img), cmap='gray')\n","        ax.set_title(title, fontsize=18)  # Set \"Actual\" or \"Predicted\" as title for each subfigure\n","        ax.get_xaxis().set_visible(False)\n","        ax.get_yaxis().set_visible(False)\n","\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","id":"2abc9e82","metadata":{"id":"2abc9e82"},"source":["## **1.5 Short answer:** Adding Layers to Autoencoder *(10 pts)*\n","Try **three** different ways of adding a layer to the encoder and a layer to the decoder and retrain it. Do they get better? Discuss what you learn from the results."]},{"cell_type":"code","execution_count":null,"id":"66cbbac6","metadata":{"id":"66cbbac6"},"outputs":[],"source":["## -- ! code required --your solution(s)\n","class Autoencoder_new(nn.Module):\n","    def __init__(self, encoding_dim):\n","        super(Autoencoder_new, self).__init__()\n","\n","        # Try different ways of adding layers to your encoder and decoder\n","        self.encoder = nn.Sequential(\n","        # Write your code in this block -----\n","\n","\n","\n","        # End of your code --------\n","\n","        )\n","        self.decoder = nn.Sequential(\n","        # Write your code in this block -----\n","\n","\n","\n","        # End of your code --------\n","        )\n","\n","    def forward(self, x):\n","        # Write your code in this block -----\n","\n","\n","\n","        # End of your code --------\n","\n","        return decoded\n","\n","\n","encoding_dim = 64\n","model_new = Autoencoder_new(encoding_dim)\n","print(model_new)\n","train_model(model_new, train_loader, criterion, optimizer, epochs=5)\n","evaluate_model(model_new, test_loader)"]},{"cell_type":"markdown","id":"cb1d0823","metadata":{"id":"cb1d0823"},"source":["Write your answer in this block\n","\n","**Answer:**\n"]},{"cell_type":"markdown","id":"18442466","metadata":{"id":"18442466"},"source":["## **1.6 Short answer:** Decoding Training Data *(10 pts)*"]},{"cell_type":"markdown","id":"8ca3e645","metadata":{"id":"8ca3e645"},"source":["Determine the mean and covariance of the samples from our training data. Now\n","draw 10 random samples from a normal distribution with that mean and\n","covariance, and feed these samples into the decoder. Do the results look\n","like images?"]},{"cell_type":"code","execution_count":null,"id":"71f2a3d0","metadata":{"id":"71f2a3d0"},"outputs":[],"source":["## -- ! code required --your solution(s)\n","\n","# Step 1: Collect samples from training data\n","\n","# Step 2: Calculate mean and covariance of the input samples\n","\n","\n","# Step 3: Sample from the normal distribution with calculated mean and covariance\n","\n","\n","# Step 4: Decode samples and visualize\n","\n","\n","# Plot the generated images\n"]},{"cell_type":"markdown","id":"9cf92626","metadata":{"id":"9cf92626"},"source":["Write your answer in this block\n","\n","**Answer:**"]},{"cell_type":"markdown","id":"8a73deab","metadata":{"id":"8a73deab"},"source":["## **1.7 Short answer:** Decoding using Mixture of Normals *(10 pts)*"]},{"cell_type":"markdown","id":"9e3b121b","metadata":{"id":"9e3b121b"},"source":["We will now model the images using a mixture of normals. For each digit class, determine the mean and covariance based on the training data.\n","Your mixture distribution that samples evenly from the ten class distributions. Now draw 10 random samples from this mixture distribution,\n","and feed these samples into the decoder. Do the results look like images?"]},{"cell_type":"code","execution_count":null,"id":"8845831d","metadata":{"id":"8845831d"},"outputs":[],"source":["## -- ! code required --your solution(s)\n","\n","\n","# Step 1: Calculate mean and covariance for each class\n","\n","# Step 2: Calculate the mean and covariance for each class\n","\n","# Step 3: Create the mixture model and draw the samples\n","\n","# Plot the generated images\n"]},{"cell_type":"markdown","id":"4009ec27","metadata":{"id":"4009ec27"},"source":["Write your answer in this block\n","\n","**Answer:**"]},{"cell_type":"markdown","id":"7e773510","metadata":{"id":"7e773510"},"source":["# **Question 2.** Attention  (*25 total points*)"]},{"cell_type":"markdown","id":"c931e371","metadata":{"id":"c931e371"},"source":["In this question we will look at how we can apply the attention mechanism to a CNN model. We will reuse the MNIST dataset from question 1. In this question we will look at how using attention mechanism with a basic CNN network can affect the performance of our classification task."]},{"cell_type":"markdown","id":"a9328bec","metadata":{"id":"a9328bec"},"source":["## **2.1 Code:** Basic Attention *(10 pts)*"]},{"cell_type":"markdown","id":"198e2f90","metadata":{"id":"198e2f90"},"source":["In this question we will provide you with the BasicCNN network that you will work with."]},{"cell_type":"markdown","id":"fc577b4d","metadata":{"id":"fc577b4d"},"source":["Let us first look at how attention is calculated:"]},{"cell_type":"markdown","id":"c0a29579","metadata":{"id":"c0a29579"},"source":["![alt text](https://github.com/ellywang66/CS541/raw/main/attention.png)"]},{"cell_type":"markdown","id":"9244cd4c","metadata":{"id":"9244cd4c"},"source":["For the attention mechanism, the components $Q$, $K$, and $V$ represent the **query**, **key**, and **value** matrices, derived from the input data. Below is a breakdown of each step:\n","\n","- **$QK^T$**:computes a **similarity score matrix** by taking the dot product between each query and key vector. This matrix measures how each element (token) should focus on every other element in the input sequence.\n","\n","- **$\\sqrt{d_k}$**: Dividing by the square root of $d_k$ scales down the dot product values to stabilize the training process by preventing large values and avoid gradient explosion.\n","\n","- **softmax**: The softmax function is applied to the scaled similarity scores by converting each value into probability.\n","\n","- **$V$**: multiply attention weight by $V$ to compute the final attention output."]},{"cell_type":"markdown","id":"d725763b","metadata":{"id":"d725763b"},"source":["For basic attention it will attend to elements in one sequence based on their relevance to a specific context or query where 𝑄\n"," comes from the target sequence and 𝐾,𝑉 are from the source sequence.For this uestion we will simply let query be a **learnable parameter**."]},{"cell_type":"code","execution_count":null,"id":"b83da69f","metadata":{"id":"b83da69f"},"outputs":[],"source":["# the BasicCNN model\n","class BasicCNN(nn.Module):\n","    def __init__(self):\n","        super(BasicCNN, self).__init__()\n","        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n","        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n","        self.pool = nn.MaxPool2d(2, 2)\n","        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n","\n","    def forward(self, x):\n","        x = self.pool(torch.relu(self.conv1(x)))\n","        x = self.pool(torch.relu(self.conv2(x)))\n","        x = x.view(-1, 64 * 7 * 7)\n","        x = torch.relu(self.fc1(x))\n","        return x"]},{"cell_type":"code","execution_count":null,"id":"a02f4055","metadata":{"id":"a02f4055"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","# Complete SimpleAttention\n","class BasicAttention(nn.Module):\n","    def __init__(self, query_dim, key_dim, value_dim, output_dim):\n","        super(BasicAttention, self).__init__()\n","\n","        ## TODO: initialize the query,key and value using linear layers where both the input and output dimension is 128.\n","\n","\n","\n","\n","    def forward(self, query_input, key_input, value_input):\n","        ## TODO: calculate Q, K, and V using  query_input, key_input, value_input.\n","\n","        #calculate attention score\n","\n","        # apply attention weights to V\n","\n","        #return attention output\n","\n","\n","\n","\n","        return output\n","\n","\n","class CNNWithBasicAttention(nn.Module):\n","    def __init__(self):\n","        super(CNNWithBasicAttention, self).__init__()\n","        self.cnn = BasicCNN()\n","        ## TODO: initialize self.attention where query, key, value and output dimension are all 128.\n","\n","\n","        # Initialize a learnable query vector for basic attention\n","\n","\n","        ## Initialize final fully connected layer for the classification head using linear layer.\n","\n","\n","\n","    def forward(self, x):\n","         ## TODO: we will pass the input x through cnn followed by the attention layer and the fully connected layer\n","\n","\n","        return x"]},{"cell_type":"code","execution_count":null,"id":"dba5b6a1","metadata":{"id":"dba5b6a1"},"outputs":[],"source":["def train_model(model, train_loader, criterion, optimizer, epochs=5):\n","    model.train()\n","    for epoch in range(epochs):\n","        running_loss = 0.0\n","        for images, labels in train_loader:\n","            optimizer.zero_grad()\n","            outputs = model(images)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","            running_loss += loss.item()\n","        print(f'Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader):.4f}')\n","\n","\n","def test_model(model, test_loader):\n","    model.eval()\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for images, labels in test_loader:\n","            outputs = model(images)\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","    print(f'Test Accuracy: {100 * correct / total:.2f}%')"]},{"cell_type":"markdown","id":"de6156ed","metadata":{"id":"de6156ed"},"source":["We can now test the performance of our model"]},{"cell_type":"code","execution_count":null,"id":"384bcbc2","metadata":{"id":"384bcbc2"},"outputs":[],"source":["# Train and evaluate the model\n","model = CNNWithBasicAttention()\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","train_model(model, train_loader, criterion, optimizer, epochs=5)\n","test_model(model, test_loader)"]},{"cell_type":"markdown","id":"0b853349","metadata":{"id":"0b853349"},"source":["## **2.2 Code:** Self-Attention *(10 pts)*"]},{"cell_type":"markdown","id":"e6016a32","metadata":{"id":"e6016a32"},"source":["In self attention the attention score is computed solely based on the input sequence without additional learned parameters. The attention socre is computed solely based on the input sequence without any additional learned parameters."]},{"cell_type":"code","execution_count":null,"id":"18a1bb15","metadata":{"id":"18a1bb15"},"outputs":[],"source":["\n","\n","# Step 2: Define a Self-Attention Layer\n","class SelfAttention(nn.Module):\n","    def __init__(self, input_dim):\n","        super(SelfAttention, self).__init__()\n","        ## TODO: initialize the query,key and value using linear layers where both the input and output dimension is 128.\n","\n","\n","\n","    def forward(self, x):\n","        ## TODO: calculate Q, K, and V using  query_input, key_input, value_input.\n","\n","        # calculate attention score\n","\n","        # multiply attention scores by the value matrix\n","\n","        # return attention output\n","\n","\n","\n","class CNNWithSelfAttention(nn.Module):\n","    def __init__(self):\n","        super(CNNWithSelfAttention, self).__init__()\n","        self.cnn = BasicCNN()\n","        ## TODO: initialize self.attention where the input dimension 128.\n","\n","        ## Initialize final fully connected layer for the classification head using linear layer.\n","\n","\n","    def forward(self, x):\n","         ## TODO: we will pass the input x through cnn followed by the attention layer and the fully connected layer\n","\n","\n","\n","\n","\n","\n","# Instantiate and train the model\n","model = CNNWithSelfAttention()\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n"]},{"cell_type":"code","execution_count":null,"id":"067f59cf","metadata":{"id":"067f59cf"},"outputs":[],"source":["# Run training and testing\n","train_model(model, train_loader, criterion, optimizer, epochs=5)\n","test_model(model, test_loader)"]},{"cell_type":"markdown","id":"2d0ad02c","metadata":{"id":"2d0ad02c"},"source":["## **2.3 Short answer:** Basic attention vs self-attention *(5 pts)*\n","\n","Compare the performance of basic attention and self-attention. Which one do you think performs better and why?"]},{"cell_type":"markdown","id":"8f21885e","metadata":{"id":"8f21885e"},"source":["Write your answer in this block\n","\n","**Answer:**"]},{"cell_type":"markdown","id":"VVXtNkBGbRCp","metadata":{"id":"VVXtNkBGbRCp"},"source":["# **Question 3** : ViT *(20 points)*\n"]},{"cell_type":"markdown","id":"fTRknF8vCW6C","metadata":{"id":"fTRknF8vCW6C"},"source":["## **3.1 Code:** Multi-head self-attention *(10 pts)*\n","Begin by implementing multiheaded self-attention. Do **not** use any `for` loops, and instead put all of the calculations into [batch matrix multiplications](https://pytorch.org/docs/stable/generated/torch.bmm.html) or [Linear layers](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html).\n","\n","Useful references include the lecture slides and the [illustrated transformer](https://jalammar.github.io/illustrated-transformer/).\n"]},{"cell_type":"code","execution_count":null,"id":"nERM0nQobT9y","metadata":{"id":"nERM0nQobT9y"},"outputs":[],"source":["from torch import nn\n","import torch\n","import math\n","\n","class MSA(nn.Module):\n","  def __init__(self, input_dim, embed_dim, num_heads):\n","    '''\n","    input_dim: Dimension of input token embeddings\n","    embed_dim: Dimension of internal key, query, and value embeddings\n","    num_heads: Number of self-attention heads\n","    '''\n","\n","    super().__init__()\n","\n","    self.input_dim = input_dim\n","    self.embed_dim = embed_dim\n","    self.num_heads = num_heads\n","\n","    ### Implement the variables\n","\n","    self.K_embed = raise NotImplementedError\n","    self.Q_embed = raise NotImplementedError\n","    self.V_embed = raise NotImplementedError\n","    self.out_embed = raise NotImplementedError\n","\n","    ### Code End\n","\n","  def forward(self, x):\n","    '''\n","    x: input of shape (batch_size, max_length, input_dim)\n","    return: output of shape (batch_size, max_length, embed_dim)\n","    '''\n","\n","    batch_size, max_length, given_input_dim = x.shape\n","    assert given_input_dim == self.input_dim\n","    assert max_length % self.num_heads == 0\n","\n","    # You shouldn't need to initialize any new modules. Everything you need is\n","    # already in __init__\n","\n","    # HINT: If you're stuck on how to handle multiple heads without for loops, try to\n","    # reshape matrix such that the batch_size is num_heads * batch_size\n","    # e.g. if you have two heads, you'd be doing self-attention twice per instance\n","    # in the batch, so you essentially have batch_size * 2\n","\n","    # HINT 2: Feel free to reference: https://d2l.ai/chapter_attention-mechanisms-and-transformers/multihead-attention.html\n","    # although make sure you understand what each command does\n","\n","    # this implementation projects KQV before splitting into multiple heads\n","    # but you can also split into multiple heads first\n","\n","    # compute KQV as a whole, embedding and\n","    x = x.reshape(batch_size * max_length, -1)\n","    K = self.K_embed(x).reshape(batch_size, max_length, self.embed_dim) # (batch_size, max_length, embed_dim)\n","    Q = raise NotImplementedError # TODO: Compute Q\n","    V = raise NotImplementedError # TODO: Compute V\n","\n","\n","    indiv_dim = self.embed_dim // self.num_heads\n","\n","\n","    ## Implement the variables\n","    # TODO: split each KQV into heads, by reshaping each into (batch_size, max_length, self.num_heads, indiv_dim)\n","    K = raise NotImplementedError\n","    Q = raise NotImplementedError\n","    V = raise NotImplementedError\n","\n","    K = K.permute(0, 2, 1, 3) # (batch_size, num_heads, max_length, embed_dim / num_heads)\n","    Q = Q.permute(0, 2, 1, 3)\n","    V = V.permute(0, 2, 1, 3)\n","\n","\n","    ## Implement the variables\n","\n","    K = raise NotImplementedError\n","    Q = raise NotImplementedError\n","    V = raise NotImplementedError\n","\n","    # transpose and batch matrix multiply\n","    K_T = K.permute(0, 2, 1) # This is our K transposed so we can do a simple batched matrix multiplication (see torch.bmm for more details and the quick solution)\n","    QK = torch.bmm(Q, K_T) # TODO: Compute the weights before dividing by square root of d (batch_size * num_heads, max_length, max_length)\n","\n","    weights = raise NotImplementedError # TODO Calculate weights by dividing everything by the square root of d (self.embed_dim)\n","    weights = raise NotImplementedError # TODO Take the softmax over the last dimension (batch_size * num_heads, max_length, max_length)\n","\n","    # TODO get weighted average... see torch.bmm for a one line solution\n","    w_V = raise NotImplementedError # weights is (batch_size * num_heads, max_length, max_length) and V is (batch_size * self.num_heads, max_length, indiv_dim), so we want the matrix multiple of weights @ V\n","\n","    # rejoin heads\n","    w_V = w_V.reshape(batch_size, self.num_heads, max_length, indiv_dim)\n","    w_V = w_V.permute(0, 2, 1, 3) # (batch_size, max_length, num_heads, embed_dim / num_heads)\n","    w_V = w_V.reshape(batch_size, max_length, self.embed_dim)\n","\n","    out = self.out_embed(w_V)\n","\n","    return out\n"]},{"cell_type":"code","execution_count":null,"id":"ZmRNX2FZeYDZ","metadata":{"id":"ZmRNX2FZeYDZ"},"outputs":[],"source":["class CNNWithMSA(nn.Module):\n","    def __init__(self):\n","        super(CNNWithMSA, self).__init__()\n","        self.cnn = BasicCNN()\n","        ## TODO: initialize 1-head MSA where the input and embedding dimension 128.\n","        self.self_attention = raise NotImplementedError\n","        ## Initialize final fully connected layer for the classification head using linear layer.\n","        self.fc_final = raise NotImplementedError\n","\n","    def forward(self, x):\n","        ## TODO: we will pass the input x through cnn followed by the attention layer and the fully connected layer\n","        ## Different from the above part, you may need to apply a trick to the variable by using unsqueeze() or squeeze().\n","        raise NotImplementedError\n","\n","\n","        return x\n"]},{"cell_type":"code","execution_count":null,"id":"nBk6pTwGCx-9","metadata":{"id":"nBk6pTwGCx-9"},"outputs":[],"source":["# Instantiate and train the model\n","model = CNNWithMSA()\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","# Run training and testing\n","train_model(model, train_loader, criterion, optimizer, epochs=5)\n","test_model(model, test_loader)"]},{"cell_type":"markdown","id":"JXg5ZrtLba09","metadata":{"id":"JXg5ZrtLba09"},"source":["## **3.2 Code:** Implement the ViT architecture *(10 pts)*\n"]},{"cell_type":"markdown","id":"LbhoBUEsGWdO","metadata":{"id":"LbhoBUEsGWdO"},"source":["You will be implementing the ViT architecture based on the \"An image is worth 16x16 words\" paper.\n","\n","Although the ViT and Transformer architecture are very similar, note a few differences:\n","\n","1. Image patches instead of discrete tokens as input.\n","2. [GELU](https://pytorch.org/docs/stable/generated/torch.nn.GELU.html) is used for the linear layers in the transformer layer (instead of ReLU)\n","3. LayerNorm before the sublayer instead of after.\n","4. Dropout after every linear layer except for KQV projections and also directly after adding positional embeddings to the patch embeddings.\n","5. Learnable [CLS] token at the beginning of the input.\n","\n","A useful reference is Figure 1 in the [paper](https://arxiv.org/pdf/2010.11929.pdf)."]},{"cell_type":"markdown","id":"U0wnlC6NbdGy","metadata":{"id":"U0wnlC6NbdGy"},"source":["First, implement a single layer:"]},{"cell_type":"code","execution_count":null,"id":"nBNICyUJbYY_","metadata":{"id":"nBNICyUJbYY_"},"outputs":[],"source":["class ViTLayer(nn.Module):\n","  def __init__(self, num_heads, input_dim, embed_dim, mlp_hidden_dim, dropout=0.1):\n","    '''\n","    num_heads: Number of heads for multi-head self-attention\n","    embed_dim: Dimension of internal key, query, and value embeddings\n","    mlp_hidden_dim: Hidden dimension of the linear layer\n","    dropout: Dropout rate\n","    '''\n","\n","    super().__init__()\n","\n","    self.input_dim = input_dim\n","    self.msa = MSA(input_dim, embed_dim, num_heads)\n","\n","    self.layernorm1 = nn.LayerNorm(embed_dim)\n","    self.w_o_dropout = nn.Dropout(dropout)\n","    self.layernorm2 = nn.LayerNorm(embed_dim)\n","    self.mlp = nn.Sequential(nn.Linear(embed_dim, mlp_hidden_dim),\n","                              nn.GELU(),\n","                              nn.Dropout(dropout),\n","                              nn.Linear(mlp_hidden_dim, embed_dim),\n","                              nn.Dropout(dropout))\n","\n","  def forward(self, x):\n","    '''\n","    x: input embeddings (batch_size, max_length, input_dim)\n","    return: output embeddings (batch_size, max_length, embed_dim)\n","    '''\n","\n","    # TODO: Fill in the code for the forward pass below\n","    # You shouldn't need to initialize any more modules, everything you need is already\n","    # in __init__\n","    # A forward function consists of:\n","    # 1) LayerNorm of x\n","    x1 = raise NotImplementedError\n","    # 2) Self-Attention on output of 1)\n","    attn1 = raise NotImplementedError\n","    # 3) Dropout\n","    attn1 = raise NotImplementedError\n","    # 4) Residual w/ original x\n","    attn1 = raise NotImplementedError\n","    # 5) LayerNorm\n","    attn1 = raise NotImplementedError\n","    # 6) MLP\n","    mlp_out = raise NotImplementedError\n","    # 7) Residual\n","    output = mlp_out + attn1\n","\n","    return output\n"]},{"cell_type":"markdown","id":"qiCtiMSWbgHw","metadata":{"id":"qiCtiMSWbgHw"},"source":["A portion of the full network is already implemented for you. Your task is to implement the preprocessing code, converting raw images into patch embeddings + positional embeddings + dropout, with a learnable CLS token at the beginning of the input.\n","\n","Note that patch embeddings are to be added to positional embeddings elementwise, so the input embedding dimensions is size embed_dim."]},{"cell_type":"code","execution_count":null,"id":"Od7ix4Y1beiw","metadata":{"id":"Od7ix4Y1beiw"},"outputs":[],"source":["class ViT(nn.Module):\n","  def __init__(self, patch_dim, image_dim, num_layers, num_heads, embed_dim, mlp_hidden_dim, num_classes, dropout):\n","    '''\n","    patch_dim: patch length and width to split image by\n","    image_dim: image length and width\n","    num_layers: number of layers in network\n","    num_heads: number of heads for multi-head attention\n","    embed_dim: dimension to project images patches to and dimension to use for position embeddings\n","    mlp_hidden_dim: hidden dimension of linear layer\n","    num_classes: number of classes to classify in data\n","    dropout: dropout rate\n","    '''\n","\n","    super().__init__()\n","    self.num_layers = num_layers\n","    self.patch_dim = patch_dim\n","    self.image_dim = image_dim\n","    self.input_dim = self.patch_dim * self.patch_dim * 3\n","    self.num_heads = num_heads\n","\n","    self.patch_embedding = nn.Linear(self.input_dim, embed_dim)\n","    self.position_embedding = nn.Parameter(torch.zeros(1, (image_dim // patch_dim) ** 2 + 1, embed_dim))\n","    self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n","    self.embedding_dropout = nn.Dropout(dropout)\n","\n","    self.encoder_layers = nn.ModuleList([])\n","    for i in range(num_layers):\n","      self.encoder_layers.append(ViTLayer(num_heads, embed_dim, embed_dim, mlp_hidden_dim, dropout))\n","\n","    self.mlp_head = nn.Linear(embed_dim, num_classes)\n","    self.layernorm = nn.LayerNorm(embed_dim)\n","\n","  def forward(self, images):\n","    '''\n","    images: raw image data (batch_size, channels, rows, cols)\n","    '''\n","\n","    # Don't hardcode dimensions (except for maybe channels = 3), use the variables in __init__.\n","    # You shouldn't need to add anything else to __init__, all of the embeddings,\n","    # dropout etc. are already initialized for you.\n","\n","    # Put the preprocessed patches in variable \"out\" with shape (batch_size, length, embed_dim).\n","\n","    # HINT: You can make image patches with .reshape\n","    # e.g.\n","    # x = torch.ones((100, 100))\n","    # x_patches = x.reshape(4, 25, 4, 25)\n","    # where you have 4 * 4 patches with each patch being 25 by 25\n","\n","    h = w = self.image_dim // self.patch_dim\n","    N = images.size(0)\n","    images = images.reshape(N, 3, h, self.patch_dim, w, self.patch_dim)\n","    images = torch.einsum(\"nchpwq -> nhwpqc\", images)\n","    patches = images.reshape(N, h * w, self.input_dim) # (batch, num_patches_per_image, patch_size_unrolled)\n","\n","    patch_embeddings = self.patch_embedding(patches)\n","    patch_embeddings = torch.cat([torch.tile(self.cls_token, (N, 1, 1)),\n","                                  patch_embeddings], dim=1)\n","    out = patch_embeddings + torch.tile(self.position_embedding, (N, 1, 1)) # We add positional embeddings to our tokens (not concatenated)\n","    out = raise NotImplementedError # TODO: Pass through our embedding dropout layer\n","\n","    # add padding s.t. input length is multiple of num_heads\n","    add_len = (self.num_heads - out.shape[1]) % self.num_heads\n","    out = torch.cat([out, torch.zeros(N, add_len, out.shape[2])], dim=1)\n","\n","    # TODO: Pass through each one of our encoder layers\n","\n","    for i in range(self.num_layers):\n","      out = raise NotImplementedError\n","\n","    # Pop off and read our classification token we added, see what the value is\n","    cls_head = raise NotImplementedError\n","    logits = raise NotImplementedError\n","    return logits\n","\n","def get_vit_tiny(num_classes=10, patch_dim=4, image_dim=32):\n","    return ViT(patch_dim=patch_dim, image_dim=image_dim, num_layers=12, num_heads=3,\n","              embed_dim=192, mlp_hidden_dim=768, num_classes=num_classes, dropout=0.1)\n","\n","def get_vit_small(num_classes=10, patch_dim=4, image_dim=32):\n","    return ViT(patch_dim=patch_dim, image_dim=image_dim, num_layers=12, num_heads=6,\n","               embed_dim=384, mlp_hidden_dim=1536, num_classes=num_classes, dropout=0.1)"]},{"cell_type":"markdown","id":"EP53yZiYDdBc","metadata":{"id":"EP53yZiYDdBc"},"source":["## **3.3 Code & Short Answer**: Train ViT *(5 pts)*"]},{"cell_type":"markdown","id":"CK_zg8MObjgI","metadata":{"id":"CK_zg8MObjgI"},"source":["Now let's train the model!\n","\n","- Try to get 60%+ accuracy. Make sure to include the final printed accuracy in your PDF file! Otherwise, you won’t receive full points.\n","- Briefly explain what you did to obtain the accuracy"]},{"cell_type":"code","execution_count":null,"id":"8LysdRf5bhsJ","metadata":{"id":"8LysdRf5bhsJ"},"outputs":[],"source":["from torch.utils.data import DataLoader, Dataset\n","import torchvision.transforms as transforms\n","import torchvision\n","import math\n","import torch.optim as optim\n","\n","cifar10_mean = torch.tensor([0.49139968, 0.48215827, 0.44653124])\n","cifar10_std = torch.tensor([0.24703233, 0.24348505, 0.26158768])\n","\n","class Cifar10Dataset(Dataset):\n","    def __init__(self, train):\n","        self.transform = transforms.Compose([\n","                                                transforms.Resize(40),\n","                                                transforms.RandomCrop(32),\n","                                                transforms.RandomHorizontalFlip(),\n","                                                transforms.ToTensor(),\n","                                                transforms.Normalize(cifar10_mean, cifar10_std)\n","                                            ])\n","        self.dataset = torchvision.datasets.CIFAR10(root='./SSL-Vision/data',\n","                                                    train=train,\n","                                                    download=True)\n","    def __len__(self):\n","        return len(self.dataset)\n","\n","    def __getitem__(self, idx):\n","        img, label = self.dataset[idx]\n","        img = self.transform(img)\n","        return img, label\n","\n","batch_size = 256\n","\n","def question_3_3():\n","\n","  trainset = Cifar10Dataset(True)\n","  trainloader = trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n","\n","\n","  testset = Cifar10Dataset(False)\n","  testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n","\n","  classes = ('plane', 'car', 'bird', 'cat',\n","            'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n","\n","  vit = get_vit_small()\n","  vit = torch.nn.DataParallel(vit)\n","\n","  learning_rate = raise NotImplementedError\n","  num_epochs = raise NotImplementedError\n","  warmup_fraction = raise NotImplementedError\n","  weight_decay = raise NotImplementedError\n","\n","  total_steps = math.ceil(len(trainset) / batch_size) * num_epochs\n","  total_steps = num_epochs\n","  warmup_steps = total_steps * warmup_fraction\n","  criterion = nn.CrossEntropyLoss()\n","  optimizer = raise NotImplementedError\n","\n","  train_losses = []\n","  test_losses = []\n","  for epoch in range(num_epochs):\n","      train_loss = 0.0\n","      train_acc = 0.0\n","      train_total = 0\n","      vit.train()\n","      for inputs, labels in trainloader:\n","          \"\"\"TODO:\n","          1. Set inputs and labels to be on device\n","          2. zero out our gradients\n","          3. pass our inputs through the ViT\n","          4. pass our outputs / labels into our loss / criterion\n","          5. backpropagate\n","          6. step our optimizeer\n","          \"\"\"\n","        raise NotImplementedError\n","\n","      train_loss = train_loss / train_total\n","      train_acc = train_acc / train_total\n","      train_losses.append(train_loss)\n","\n","      test_loss = 0.0\n","      test_acc = 0.0\n","      test_total = 0\n","      vit.eval()\n","      with torch.no_grad():\n","          for inputs, labels in testloader:\n","              inputs = inputs.to(device)\n","              labels = labels.to(device)\n","\n","              outputs = vit(inputs)\n","              loss = criterion(outputs, labels.long())\n","\n","              test_loss += loss.item() * inputs.shape[0]\n","              test_acc += torch.sum((torch.argmax(outputs, dim=1) == labels)).item()\n","              test_total += inputs.shape[0]\n","      test_loss = test_loss / test_total\n","      test_acc = test_acc / test_total\n","      test_losses.append(test_loss)\n","\n","      print(f'[{epoch + 1:2d}] train loss: {train_loss:.3f} | train accuracy: {train_acc:.3f} | test_loss: {test_loss:.3f} | test_accuracy: {test_acc:.3f}')\n","\n","  print('Finished Training')\n","question_3_3()"]},{"cell_type":"markdown","id":"eSChVMgzGIpF","metadata":{"id":"eSChVMgzGIpF"},"source":["Write your answer in this block\n","\n","**Answer:**"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"hide_input":false,"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{"height":"calc(100% - 180px)","left":"10px","top":"150px","width":"279px"},"toc_section_display":true,"toc_window_display":true}},"nbformat":4,"nbformat_minor":5}
